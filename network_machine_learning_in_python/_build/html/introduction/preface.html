
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Front Matter &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Matrix Representations Of Networks" href="../representations/ch4/matrix-representations.html" />
    <link rel="prev" title="HANDS-ON NETWORK MACHINE LEARNING WITH GRASPOLOGIC AND SCIKIT-LEARN" href="../coverpage.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../coverpage.html">
   HANDS-ON NETWORK MACHINE LEARNING WITH GRASPOLOGIC AND SCIKIT-LEARN
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Front Matter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../representations/ch4/matrix-representations.html">
   Matrix Representations Of Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../representations/ch6/why-embed-networks.html">
   Why embed networks?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../representations/ch6/spectral-embedding.html">
   Spectral Embedding Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../representations/ch6/multigraph-representation-learning.html">
   Multiple-Network Representation Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../representations/ch6/joint-representation-learning.html">
   Joint Representation Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applications/ch8/single-vertex-nomination.html">
   Single-Network Vertex Nomination
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applications/ch8/out-of-sample.html">
   Out-of-sample Embedding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applications/ch10/anomaly-detection.html">
   Anomaly Detection For Timeseries of Networks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/introduction/preface.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/loftusa/thesis/master?urlpath=tree/network_machine_learning_in_python/introduction/preface.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgements">
   Acknowledgements
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dedication">
   <strong>
    Dedication
   </strong>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contents">
   Contents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#list-of-figures">
   List of Figures
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="front-matter">
<h1>Front Matter<a class="headerlink" href="#front-matter" title="Permalink to this headline">¶</a></h1>
<div class="section" id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h2>
<p>This thesis is a general overview of spectral methods on networks, and how you can use tools from a network’s eigenspace to understand and explain the network more deeply.</p>
<p>Networks are some of the most fundamental building blocks of the universe. Atoms and molecules are connected to each other with chemical bonds. Neurons connect to each other through synapses, and the different parts of the brain connect to each other through groups of neurons interacting with each other. At a larger level, we are interconnected with other humans through social networks, and our economy is a global, interconnected trade network. The Earth’s food chain is an ecological network, and larger still, every object with mass in the universe is connected to every other object through a gravitational network.</p>
<p>This thesis covers the fundamentals of spectral methods with respect to network data science, focusing on developing intuition on networks as statistical objects, while paired with relevant Python code.</p>
</div>
<div class="section" id="acknowledgements">
<h2>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Permalink to this headline">¶</a></h2>
<p>Big thanks to everybody who has been reading the thesis as I write and giving feedback. This list includes Dax Pryce, Ross Lawrence, Geoff Loftus, Alexandra McCoy, Olivia Taylor Peter Brown, Sambit Panda, Eric Bridgeford, Josh Vogelstein, and Ali Sad-Aldin.</p>
<p>I am grateful to my advisor, Joshua Vogelstein, for his insights and strong feedback. The value he puts on clarity and simplicity in any mathematical model has been an enormous help throughout this process.</p>
<p>I am also especially grateful to Eric Bridgeford, who has been giving me constant feedback throughout the writing process. I would be lost in a sea of papers without his help.</p>
</div>
<div class="section" id="dedication">
<h2><strong>Dedication</strong><a class="headerlink" href="#dedication" title="Permalink to this headline">¶</a></h2>
<p>This thesis is dedicated to my father, Geoffrey Loftus, for teaching me the value of rigor in science and for being a resoundingly positive role model throughout my life, and to my mother, Susan Loftus, for teaching me to never give up in the face of adversity.</p>
</div>
<div class="section" id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Permalink to this headline">¶</a></h2>
<p><strong>Abstract</strong><br />
<strong>Acknowledgements</strong><br />
<strong>List of Figures</strong><br />
<strong>1. Matrix Representations of Networks</strong><br />
The Adjacency Matrix<br />
The Incidence Matrix<br />
The Oriented Incidence Matrix<br />
The Degree Matrix<br />
The Laplacian Matrix<br />
<strong>2. Why Embed Networks?</strong><br />
High Dimensionality of Network Data<br />
Latent Estimation<br />
The Latent Position Matrix<br />
Edge Probability Estimation
Block Probability Matrices<br />
Geometry of Latent Positions<br />
<strong>3. Spectral Embedding Methods</strong><br />
Singular Vectors and Singular Value Decomposition<br />
Breaking Down the Laplacian<br />
Matrix Rank<br />
Sums of Rank One Matrices<br />
Laplacian Approximation Through Summation<br />
Increased Usefulness of Approximation with Larger Networks<br />
Matrix Rank and Spectral Embedding<br />
Dimensionality Estimation<br />
The Two-Truths Phenomenon<br />
<strong>4. Multiple-Network Representation Learning</strong><br />
Data Generation<br />
Simple Embedding Methods on Multiple Networks<br />
Averaging Separately<br />
Averaging Together<br />
Different Types of Multi-Network Representation Learning<br />
Network Combination: Together<br />
Network Combination: Separate<br />
Embedding Combination<br />
Multiple-Adjacency Spectral Embedding<br />
Overview of MASE
Data Generation<br />
Embedding<br />
Combining Embeddings<br />
Joint Embedding of Combinations<br />
Score Matrices<br />
Omnibus Embedding<br />
OMNI on Four Networks<br />
Overview of OMNI<br />
The Omnibus Matrix<br />
Embedding the Omnibus Matrix<br />
Using the Omnibus Embedding<br />
<strong>5. Joint Representation Learning</strong><br />
Data Generation<br />
Covariates<br />
Covariate-Assisted Spectral Embedding<br />
Weight Exploration<br />
Weight Estimation<br />
Omnibus Joint Embedding<br />
MASE Joint Embedding<br />
<strong>6. Single-Network Vertex Nomination</strong><br />
Spectral Vertex Nomination<br />
Finding a Single Set of Nominations<br />
Nominations for Each Node<br />
<strong>7. Out-of-Sample Embedding</strong><br />
Data Generation<br />
Probability Vector Estimation<br />
Inversion of Probability Vector Estimation<br />
The Moore-Penrose Pseudoinverse<br />
Using the Pseudoinverse for Out-of-Sample Estimation<br />
<strong>8. Anomaly Detection for Timeseries of Networks</strong><br />
Simulating Timeseries Data<br />
Approaches for Anomaly Detection<br />
Detecting if the First Time-Point is an Anomaly<br />
Hypothesis Testing a Test Statistic<br />
Bootstrapped Distribution Estimation<br />
P-Value Estimation<br />
Testing the Remaining Time-Points<br />
The Distribution of the Bootstrapped Test Statistic</p>
</div>
<div class="section" id="list-of-figures">
<h2>List of Figures<a class="headerlink" href="#list-of-figures" title="Permalink to this headline">¶</a></h2>
<p>1.1 A Three-Node Network<br />
1.2 Adjacency Matrix and Layout Plot<br />
1.2 Incidence Matrix and Layout Plot<br />
1.3 Oriented Incidence Matrix and Layout Plot<br />
2.1 Euclidean data represented as a data matrix and represented in Euclidean space<br />
2.2 Clustered data after K-Means<br />
2.3 A Network With Two Groups<br />
2.4 Latent Position Estimation<br />
2.5 Estimated and True Block Probability Matrices<br />
2.6 Geometry of Latent Positions<br />
3.1 The Spectral Embedding Algorithm<br />
3.2 A Simple Network<br />
3.3 The Laplacian Is Just a Function of the Adjacency Matrix<br />
3.4 Decomposing our Simple Laplacian into Eigenvectors and Eigenvalues with SVD<br />
3.5 We Can Recreate our Simple Laplacian by Summing All the Low-Rank Matrices
3.6 The Sum of an Increasing Number of Low-Rank Matrices<br />
3.7 Summing Only Two Low-Rank Matrices Approximates the Normalized Laplacian<br />
3.8 The Latent Position Matrix<br />
3.9 Low-Rank Matrices Contain the Same Information As Columns of the Latent Position Matrix<br />
3.10 Expressing the Sum With Columns of the Latent Position Matrix<br />
3.11 Scree Plot<br />
3.12 The Adjacency Spectral Embedding<br />
3.13 The Laplacian Spectral Embedding<br />
3.14 Affinity vs Core-Periphery Structure<br />
4.1 Different Sets of Brain Networks<br />
4.2 Averaged Embedded Networks<br />
4.3 Clustering with GMM<br />
4.4 Embedding when we Average Everything Together<br />
4.5 Network Comparison
4.6 Averaged Brain Network<br />
4.7 Combined Network Group Embedding<br />
4.8 Separate Network Group Embedding<br />
4.9 Combined Embedding<br />
4.10 MASE Embedding On Network Groups<br />
4.11 Four Different Networks<br />
4.12 ASE on Four Networks<br />
4.13 Latent Position Matrices for Four Embeddings<br />
4.14 Combined Embeddings For Four Networks<br />
4.15 Visualizing the Joint Embedding<br />
4.16 MASE Embedding<br />
4.17 Score Matrices and Edge Probabilities<br />
4.18 ASE for Four Networks<br />
4.19 Omnibus Embedding for Four Networks<br />
4.20 The Omnibus Matrix for Two Networks<br />
4.21 Full Omnibus Matrix for All Four Networks<br />
4.22 Latent Positions for the Omnibus Embedding in Matrix Form<br />
4.23 Latent Positions for the Omnibus Embedding in Euclidean Space<br />
4.24 Mouse Networks Corresponding to a Single Node after Omnibus Embedding
5.1 Stochastic Block Model with Three Communities<br />
5.2 Laplacian-Embedded Latent Positions<br />
5.3 Covariate Visualization<br />
5.4 Laplacian and Covariates<br />
5.5 Embedding Without Weights<br />
5.6 Comparison of Embeddings for Different Weights on <span class="math notranslate nohighlight">\(YY^\top\)</span><br />
5.7 Embedding With Weights<br />
5.8 The Benefit Of Using Two Types of Information<br />
5.9 Embedding with Graspologic<br />
5.10 Omni Embedding for Topology and Covariates<br />
5.11 MASE Joint Embedding<br />
6.1 Latent Positions and Seeds for an SBM With Three Communities<br />
6.2 Centroid for Seed Latent Positions<br />
6.3 Nomination List<br />
6.4 Nomination List: Network Plot<br />
6.5 Nominations for Each Seed Node<br />
7.1 Adjacency Matrix and Vector For Additional Node<br />
7.2 Latent Positions for Original Network<br />
7.3 Estimated Probability Vector for First Node<br />
7.4 A Noninvertible Linear Transformation<br />
7.5 The Best Approximation The Pseudoinverse Can Do<br />
7.6 Estimating the Out-of-Sample Latent Position<br />
7.7 Latent Positions with Out-of-Sample Estimate<br />
8.1 Network Timeseries Data<br />
8.2 Distribution of test statistics with the same latent positions<br />
8.3 Test Statistics for Each Timeseries<br />
8.4 Distribution of test statistics with the same latent positions<br />
8.5 Distribution Comparison for Bootstrapped and True Test Statistic</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./introduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../coverpage.html" title="previous page">HANDS-ON NETWORK MACHINE LEARNING WITH GRASPOLOGIC AND SCIKIT-LEARN</a>
    <a class='right-next' id="next-link" href="../representations/ch4/matrix-representations.html" title="next page">Matrix Representations Of Networks</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alexander Russell Loftus<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>